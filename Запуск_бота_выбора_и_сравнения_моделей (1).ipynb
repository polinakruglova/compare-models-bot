{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVB6y9-pjsxL"
      },
      "source": [
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NztNvmESYA6Z",
        "outputId": "edd85fd6-2b88-4ab4-c6d4-fd1d932b7bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pydantic 2.5.3\n",
            "Uninstalling pydantic-2.5.3:\n",
            "  Successfully uninstalled pydantic-2.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiogram 3.4.1 requires pydantic<2.6,>=2.4.1, but you have pydantic 2.7.4 which is incompatible.\n",
            "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 2.7.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.24 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.5.3 which is incompatible.\n",
            "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y pydantic\n",
        "!pip install -q pydantic==2.7.4\n",
        "!pip install -q aiogram==3.4.1 python-dotenv openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-IyKvyxjy9o"
      },
      "source": [
        "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª—é—á–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG0ABUGuXI6o"
      },
      "outputs": [],
      "source": [
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(\"TOKEN=YOUR_TOKEN\")\n",
        "    f.write(\"OPENAI_API_KEY=YOUR_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJO7fd2zXPQc",
        "outputId": "560c758e-3b57-4a85-8c89-00da7116f523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKEN: 7938278322:AAEN6dxb_XG5h536E9SJTY8YNyhAX0K1cpY\n",
            "OPENAI_API_KEY: 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\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "print(\"TOKEN:\", os.getenv(\"TOKEN\"))\n",
        "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thbW6_9RdNqf"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "TOKEN = os.getenv(\"TOKEN\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6kpDrW_lu5y"
      },
      "source": [
        "–ù–ï–û–ë–•–û–î–ò–ú–û –°–ö–ê–ß–ê–¢–¨ –ë–ê–ó–£ –î–ê–ù–ù–´–• database.db (–ü–†–ò–ö–†–ï–ü–õ–ï–ù–ê –í –ü–ê–ü–ö–ï) –í –°–í–û–ô –î–ò–°–ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EizAqkpPiG0a",
        "outputId": "52e2d30f-b7de-4883-fda0-28a52ef437cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHHP91WeiPu2",
        "outputId": "e4a62cab-e1c0-446d-9d09-5e46faf757a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/database.db': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp /content/database.db /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rvfzxASmEpE"
      },
      "source": [
        "–ö–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è –±–æ—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hqfeq4OQ8z2"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import sqlite3\n",
        "import difflib\n",
        "from openai import OpenAI\n",
        "from aiogram import Bot, Dispatcher, types, Router\n",
        "from aiogram.filters import CommandStart\n",
        "from aiogram.fsm.context import FSMContext\n",
        "from aiogram.fsm.state import StatesGroup, State\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞\n",
        "bot = Bot(token=TOKEN)\n",
        "dp = Dispatcher()\n",
        "\n",
        "# –†–æ—É—Ç–µ—Ä –¥–ª—è –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π\n",
        "user_private_router = Router()\n",
        "\n",
        "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "def connect_db():\n",
        "    return sqlite3.connect(\"/content/drive/MyDrive/database.db\")\n",
        "\n",
        "# –°–æ—Å—Ç–æ—è–Ω–∏—è FSM\n",
        "class ModelSelection(StatesGroup):\n",
        "    choosing_model = State()\n",
        "    confirming_choice = State()\n",
        "\n",
        "# –•—Ä–∞–Ω–∏–ª–∏—â–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
        "user_selected_models = {}\n",
        "\n",
        "# –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É\n",
        "def search_models(query):\n",
        "    conn = connect_db()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"SELECT name FROM models WHERE name LIKE ?\", (f\"%{query}%\",))\n",
        "    results = [row[0] for row in cursor.fetchall()]\n",
        "    if results:\n",
        "        conn.close()\n",
        "        return results\n",
        "\n",
        "    cursor.execute(\"SELECT name FROM models\")\n",
        "    all_models = [row[0] for row in cursor.fetchall()]\n",
        "    conn.close()\n",
        "\n",
        "    closest_matches = difflib.get_close_matches(query, all_models, n=5, cutoff=0.5)\n",
        "    return closest_matches if closest_matches else None\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏\n",
        "def is_model_in_db(model_name):\n",
        "    try:\n",
        "        conn = connect_db()\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT id FROM models WHERE name = ?\", (model_name,))\n",
        "        result = cursor.fetchone()\n",
        "        conn.close()\n",
        "        return result is not None\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"–û—à–∏–±–∫–∞ –ë–î:\", e)\n",
        "        return False\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫\n",
        "def get_model_specifications(model_name):\n",
        "    try:\n",
        "        conn = connect_db()\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT specification_name, specification_value\n",
        "            FROM specifications\n",
        "            WHERE model_id = (SELECT id FROM models WHERE name = ?)\n",
        "        \"\"\", (model_name,))\n",
        "        result = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return result\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"–û—à–∏–±–∫–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫:\", e)\n",
        "        return []\n",
        "\n",
        "# –û–±—Ä–∞—â–µ–Ω–∏–µ –∫ GPT\n",
        "async def chat_with_gpt(prompt):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(\"–û—à–∏–±–∫–∞ OpenAI:\", e)\n",
        "        return \"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ AI.\"\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "async def analyze_single_model(model_name):\n",
        "    if not is_model_in_db(model_name):\n",
        "        return f\"–ú–æ–¥–µ–ª—å '{model_name}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ.\"\n",
        "\n",
        "    specs = get_model_specifications(model_name)\n",
        "    if not specs:\n",
        "        return f\"–ù–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ '{model_name}'.\"\n",
        "\n",
        "    specs_text = \", \".join([f\"{name}: {value}\" for name, value in specs])\n",
        "    prompt = f\"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–æ–¥–µ–ª—å '{model_name}' —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: {specs_text}.\"\n",
        "    analysis = await chat_with_gpt(prompt)\n",
        "    return f\"–ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ '{model_name}':\\n{analysis}\"\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
        "async def compare_models(model_names):\n",
        "    models_not_found = [model for model in model_names if not is_model_in_db(model)]\n",
        "    if models_not_found:\n",
        "        return f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –±–∞–∑–µ: {', '.join(models_not_found)}\"\n",
        "\n",
        "    comparison_data = {\n",
        "        model: dict(get_model_specifications(model)) for model in model_names\n",
        "    }\n",
        "\n",
        "    prompt = f\"–°—Ä–∞–≤–Ω–∏ –º–æ–¥–µ–ª–∏: {comparison_data}\"\n",
        "    comparison = await chat_with_gpt(prompt)\n",
        "    return f\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:\\n{comparison}\"\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
        "@user_private_router.message(CommandStart())\n",
        "async def send_welcome(message: types.Message, state: FSMContext):\n",
        "    user_selected_models[message.from_user.id] = []\n",
        "    await message.reply(\"–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–º–æ–∂–Ω–æ —Å –æ—à–∏–±–∫–∞–º–∏).\")\n",
        "    await state.set_state(ModelSelection.choosing_model)\n",
        "\n",
        "@user_private_router.message(ModelSelection.choosing_model)\n",
        "async def handle_model_input(message: types.Message, state: FSMContext):\n",
        "    user_query = message.text.strip()\n",
        "    if user_query == \".\":\n",
        "        models = user_selected_models.get(message.from_user.id, [])\n",
        "        if models:\n",
        "            if len(models) == 1:\n",
        "                specs = get_model_specifications(models[0])\n",
        "                text = \"\\n\".join([f\"{name}: {value}\" for name, value in specs])\n",
        "                await message.reply(f\"üì± –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ *{models[0]}*:\\n{text}\", parse_mode=\"Markdown\")\n",
        "            else:\n",
        "                comparison = await compare_models(models)\n",
        "                await message.reply(comparison)\n",
        "        else:\n",
        "            await message.reply(\"–í—ã –Ω–µ –≤—ã–±—Ä–∞–ª–∏ –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏.\")\n",
        "\n",
        "        user_selected_models[message.from_user.id] = []\n",
        "        await state.clear()\n",
        "        await message.reply(\"–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ ‚Äî /start.\")\n",
        "        return\n",
        "\n",
        "    found = search_models(user_query)\n",
        "    if found:\n",
        "        await state.update_data(found_models=found)\n",
        "        text = \"–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å, –≤–≤–µ–¥—è –µ—ë –Ω–æ–º–µ—Ä:\\n\" + \"\\n\".join([f\"{i+1}. {m}\" for i, m in enumerate(found)])\n",
        "        await message.reply(text)\n",
        "        await state.set_state(ModelSelection.confirming_choice)\n",
        "    else:\n",
        "        await message.reply(\"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.\")\n",
        "\n",
        "@user_private_router.message(ModelSelection.confirming_choice)\n",
        "async def handle_model_selection(message: types.Message, state: FSMContext):\n",
        "    user_data = await state.get_data()\n",
        "    found_models = user_data.get(\"found_models\", [])\n",
        "\n",
        "    try:\n",
        "        idx = int(message.text.strip()) - 1\n",
        "        if 0 <= idx < len(found_models):\n",
        "            selected = found_models[idx]\n",
        "            user_selected_models[message.from_user.id].append(selected)\n",
        "            await message.reply(f\"‚úÖ –í—ã –≤—ã–±—Ä–∞–ª–∏: {selected}\\n–í–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ '.' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.\")\n",
        "            await state.set_state(ModelSelection.choosing_model)\n",
        "        else:\n",
        "            await message.reply(\"–ù–æ–º–µ—Ä –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞.\")\n",
        "    except ValueError:\n",
        "        await message.reply(\"–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –∏–∑ —Å–ø–∏—Å–∫–∞.\")\n",
        "\n",
        "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–∞\n",
        "dp.include_router(user_private_router)\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞\n",
        "async def main():\n",
        "    await bot.delete_webhook(drop_pending_updates=True)\n",
        "    await dp.start_polling(bot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIBg3BrwmlMn"
      },
      "source": [
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ–ª–µ–≥—Ä–∞–º–º –±–æ—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDpCalFydcOe"
      },
      "outputs": [],
      "source": [
        "await main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}